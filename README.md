# ğŸ§  Violence Detection using Qwen2-VL + MobileNet-LSTM
> ğŸ¥ **CCTV ì˜ìƒ ì† í­ë ¥ ì¥ë©´ì„ ìë™ ì¸ì‹í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ëª¨ë¸**
>
> ë³¸ í”„ë¡œì íŠ¸ëŠ” **ë‹¤ì¤‘ í”„ë ˆì„ ë¶„ì„ ê¸°ë°˜ í­ë ¥ ê°ì§€ ì‹œìŠ¤í…œ**ìœ¼ë¡œ,
> MobileNet + LSTM ëª¨ë¸ë¡œ í•™ìŠµëœ í­ë ¥ ë¶„ë¥˜ ëª¨ë¸ê³¼
> Qwen2-VL ë¹„ì „ ì–¸ì–´ ëª¨ë¸ì„ ì´ìš©í•´ **ì‚¬ê±´ ìš”ì•½ ë¬¸ì¥ ìë™ ìƒì„±**ê¹Œì§€ ìˆ˜í–‰í•©ë‹ˆë‹¤.
---
## ğŸš€ Project Overview
| í•­ëª© | ì„¤ëª… |
|------|------|
| **í”„ë¡œì íŠ¸ëª…** | Violence Detection |
| **ëª©í‘œ** | CCTV ë˜ëŠ” ì¼ë°˜ ì˜ìƒì—ì„œ í­ë ¥ ì¥ë©´ì„ ìë™ìœ¼ë¡œ íƒì§€í•˜ê³  ì‚¬ê±´ ê°œìš”ë¥¼ ìš”ì•½ |
| **í•µì‹¬ ê¸°ìˆ ** | MobileNetV2 + BiLSTM, Qwen2.5-VL-7B, HuggingFace Inference API |
| **í”„ë ˆì„ ë‹¨ìœ„ ì…ë ¥** | (16, 64, 64, 3) â†’ 16ì¥ì˜ ì—°ì† í”„ë ˆì„ ì…ë ¥ |
| **ë°ì´í„°ì…‹** | [Real-Life Violence Situations Dataset (Kaggle)](https://www.kaggle.com/mohamedmustafa/real-life-violence-situations-dataset) |
---
## ğŸ§© System Pipeline
ğŸ¥ CCTV / Video Input
â†“
ğŸ§± Frame Extraction (16 frames per sequence)
â†“
ğŸ‘ï¸ MobileNetV2 â†’ Feature Extraction
â†“
ğŸ§  BiLSTM â†’ Temporal Motion Analysis
â†“
âš–ï¸ Dense Layers â†’ Violence / NonViolence Classification
â†“
ğŸ—£ï¸ Qwen2-VL â†’ ì‚¬ê±´ ìš”ì•½ë¬¸ ìë™ ìƒì„±

---

## âš™ï¸ Model Structure

| Stage | Layer | Output Shape | Description |
|--------|-------|---------------|--------------|
| Input | Frames (16, 64, 64, 3) | - | 16í”„ë ˆì„ RGB ì˜ìƒ ì…ë ¥ |
| CNN | MobileNetV2 (pretrained) | (16, 7Ã—7Ã—1280) | í”„ë ˆì„ë³„ íŠ¹ì§• ì¶”ì¶œ |
| Flatten + Dropout | - | (16, 1280) | íŠ¹ì§• ë²¡í„° ì •ê·œí™” |
| BiLSTM | 32 units Ã— 2 | (64,) | ì‹œê°„ íë¦„ì— ë”°ë¥¸ ì›€ì§ì„ í•™ìŠµ |
| Dense Layers | 256 â†’ 128 â†’ 64 â†’ 32 | - | ê³ ì°¨ì› íŠ¹ì§• ì••ì¶• |
| Output | Softmax(2) | (2,) | Violence / NonViolence í™•ë¥  |

---

## ğŸ§  Qwen2-VL ê¸°ë°˜ ì‚¬ê±´ ìš”ì•½

ëª¨ë¸ì€ í­ë ¥ ê°ì§€ í›„, í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ  
**6í•˜ì›ì¹™(ëˆ„ê°€, ì–´ë””ì„œ, ë¬´ì—‡ì„, ì–´ë–»ê²Œ, ì™œ)** í˜•íƒœì˜ ì‚¬ê±´ ìš”ì•½ë¬¸ì„ ìë™ ìƒì„±í•©ë‹ˆë‹¤.

ì˜ˆì‹œğŸ‘‡  
ğŸ‘€ Input: 8ì¥ CCTV ì—°ì† í”„ë ˆì„
ğŸ§¾ Output:
â€œí•œ ë‚¨ì„±ì´ ë„ë¡œë³€ì—ì„œ ìƒëŒ€ë°©ì„ ë°€ì¹˜ë©° í­í–‰í•˜ëŠ” ì¥ë©´ìœ¼ë¡œ ë³´ì´ë©°,
ì£¼ë³€ì€ ì™¸ë¶€ ë„ë¡œ í™˜ê²½ìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. ì‚¬ê±´ì€ ì¦‰ì‹œ ì‹ ê³ ê°€ í•„ìš”í•©ë‹ˆë‹¤.â€

- **ì‚¬ìš© ëª¨ë¸:** `Qwen/Qwen2.5-VL-7B-Instruct`
- **API:** HuggingFace Inference via `openai` compatible endpoint  
- **Base64 Encoding:** ë¡œì»¬ ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ì¸ì½”ë”©í•˜ì—¬ API í˜¸ì¶œ  

---

## ğŸ“Š Training Performance

| Metric | Score |
|--------|-------|
| **Train Accuracy** | 99.9% |
| **Validation Accuracy** | 93.9% |
| **Test Accuracy** | 96.0% |
| **Best Val Loss** | 0.35 |

âœ… **Loss Function:** Categorical Crossentropy  
âœ… **Optimizer:** SGD  
âœ… **Batch Size:** 8  
âœ… **Epochs:** 50  

---

---

## ğŸ” Example Results

| Input ì˜ìƒ | Predicted | Confidence |
|--------------|------------|-------------|
| `V_341.mp4` | Violence | 0.99998 |
| `NV_112.mp4` | NonViolence | 0.99993 |

ğŸ“„ **Qwen2-VL ì‚¬ê±´ ìš”ì•½ ê²°ê³¼**
í•œ ë‚¨ì„±ì´ ë„ë¡œì—ì„œ ìƒëŒ€ë°©ì„ ì†ìœ¼ë¡œ ë°€ì¹˜ë©° í­í–‰í•˜ëŠ” ëª¨ìŠµì´ í¬ì°©ë˜ì—ˆìŠµë‹ˆë‹¤.
í˜„ì¥ì€ ë„ë¡œë³€ ì™¸ë¶€ë¡œ ë³´ì´ë©°, ì¦‰ê°ì ì¸ ì œì§€ê°€ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤.

---

## ğŸ§° Tech Stack

- **Python 3.9**
- **TensorFlow / Keras**
- **MobileNetV2 (ImageNet pretrained)**
- **HuggingFace Hub + Qwen2-VL**
- **OpenCV, NumPy, Matplotlib**
- **Scikit-learn**

---

## ğŸ“¦ Installation & Run

```bash
# Clone the repo
git clone https://github.com/fortuneSeven/Violence-Detection.git
cd Violence-Detection

# Create environment
conda create -n violence python=3.9
conda activate violence

# Install dependencies
pip install -r requirements.txt

# Run Jupyter notebook
jupyter notebook

ğŸ§­ Future Work
	â€¢	ğŸ” Real-time inference via CCTV live feed
	â€¢	ğŸ’¡ Add attention layer (Temporal or Spatial)
	â€¢	ğŸ§© Integrate TFLite for edge deployment
	â€¢	ğŸ—£ï¸ Multilingual event description generation

ğŸ§‘â€ğŸ’» Author

Jeong Jihoon (ì •ì§€í›ˆ)
Department of Artificial Intelligence, Kyonggi University
ğŸ“§ GitHub: fortuneSeven
